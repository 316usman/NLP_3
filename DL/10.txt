SCTNet: Single-Branch CNN with Transformer Semantic Information for
Real-Time Segmentation

Zhengze Xu1*, Dongyue Wu1, Changqian Yu2, Xiangxiang Chu2, Nong Sang1, Changxin Gao1‚Ä†
1National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence
and Automation, Huazhong University of Science and Technology,
2Meituan
{zhengzexu, dongyue wu nsang, cgao}@hust.edu.cn, y-changqian@outlook.com, cxxgtxy@gmail.com

3
2
0
2
c
e
D
8
2

]

V
C
.
s
c
[

1
v
1
7
0
7
1
.
2
1
3
2
:
v
i
X
r
a

Abstract

Recent real-time semantic segmentation methods usually
adopt an additional semantic branch to pursue rich long-range
context. However, the additional branch incurs undesirable
computational overhead and slows inference speed. To elimi-
nate this dilemma, we propose SCTNet, a single branch CNN
with transformer semantic information for real-time segmen-
tation. SCTNet enjoys the rich semantic representations of an
inference-free semantic branch while retaining the high ef-
ficiency of lightweight single branch CNN. SCTNet utilizes
a transformer as the training-only semantic branch consid-
ering its superb ability to extract long-range context. With
the help of the proposed transformer-like CNN block CF-
Block and the semantic information alignment module, SCT-
Net could capture the rich semantic information from the
transformer branch in training. During the inference, only the
single branch CNN needs to be deployed. We conduct exten-
sive experiments on Cityscapes, ADE20K, and COCO-Stuff-
10K, and the results show that our method achieves the new
state-of-the-art performance. The code and model is available
at https://github.com/xzz777/SCTNet.

1

Introduction

As a fundamental task in computer vision, semantic segmen-
tation aims to assign a semantic class label to each pixel in
the input image. It plays a vital role in autonomous driving,
medical image processing, mobile applications, and many
other fields. In order to achieve better segmentation perfor-
mance, recent semantic segmentation methods pursue abun-
dant long-range context. Different methods have been pro-
posed to capture and encode rich contextual information,
including large receptive fields (Chen et al. 2014, 2017,
2018), multi-scale feature fusion (Ronneberger, Fischer, and
Brox 2015; Zhao et al. 2017), self-attention mechanism (Fu
et al. 2019; Huang et al. 2019; Yuan et al. 2018; Zhao et al.
2018b; Dosovitskiy et al. 2020), etc. Among them, the self-
attention mechanism, as an essential component of trans-
formers, has been proven to have a remarkable ability to
model long-range context. Although these works improve
significantly, they usually lead to high computational costs.

*Work strenthened during an internship at Meituan.
‚Ä†Corresponding author

Copyright ¬© 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: The speed-accuracy performance on Cityscapes
validation set. Our methods are presented in red starts,
while others are presented in blue dots. Our SCTNet estab-
lishes a new state-of-the-art speed-accuracy trade-off.

Note that self-attention-based works even have square com-
putation complexity with respect to image resolution, which
significantly increases latency in processing high-resolution
images. These limitations hinder their application in real-
time semantic segmentation.

Many recent real-time works adopt a bilateral architecture
to extract high-quality semantic information at a fast speed.
BiSeNet (Yu et al. 2018) proposes a bilateral network to sep-
arate the detailed spatial features and ample contextual infor-
mation at early stages and process them in parallel, which is
shown in Figure 2(a). Following BiseNet (Yu et al. 2018),
BiSeNetV2 (Yu et al. 2021) and STDC (Fan et al. 2021)
make further efforts to strengthen the capability to extract
rich long-range context or reduce the computational costs of
the spatial branch. To balance inference speed and accuracy,
DDRNet (Pan et al. 2022), RTFormer (Wang et al. 2022),
and SeaFormer (Wan et al. 2023) adopt a feature-sharing ar-
chitecture that divides spatial and contextual features at the
deep stages, as shown in Figure 2(b). However, these meth-
ods introduce dense fusion modules between two branches
to boost the semantic information of extracted features. In
conclusion, all these bilateral methods suffer from limited
inference speed and high computational costs due to the ad-
ditional branch and multiple fusion modules.

To eliminate the aforementioned dilemma, we propose
a single-branch CNN with transformer semantic informa-

STDC2-75PIDNet-MAFFormer-B-100AFFormer-B-75SFNet-R18RTFormer-SRTFormer-BTopFormer-B-100SeaFormer-B-100SCTNet-S-75SCTNet-B-100SCTNet-B-75SCTNet-B-50BiSeNetV2-LSegNext-T-100SegNext-T-75DDRNet23DDRNet23SlimPIDNet-S757677787980812030405060708090100110120130140150Accuracy (mIoU%)Inference Speed (FPS) 
 
 
 
 
 
effective way.

‚Ä¢ Extensive experimental results show that the proposed
SCTNet outperforms existing state-of-the-art meth-
ods for real-time semantic segmentation on Cityscapes,
ADE20K, and COCO-Stuff-10K. SCTNet provides a
new view of boosting the speed and improving the per-
formance for real-time semantic segmentation

2 Related Work
Semantic Segmentation. FCN (Long, Shelhamer, and Dar-
rell 2015) leads to the tendency to utilize CNN for se-
mantic segmentation. Following FCN, a series of improved
CNN-based semantic segmentation methods are proposed.
DeepLab (Chen et al. 2017) enlarges the receptive field with
dilated convolution. PSPNet (Zhao et al. 2017), U-Net (Ron-
neberger, Fischer, and Brox 2015), and RefineNet (Lin et al.
2017) fuse different level feature representations to capture
multi-scale context. Some methods (Fu et al. 2019; Huang
et al. 2019; Yuan et al. 2018; Zhao et al. 2018b)propose
various attention modules to improve segmentation per-
formance. In recent years, transformer has been adopted
for semantic segmentation and shows promising perfor-
mance. SETR (Zheng et al. 2021) directly applies the vi-
sion transformer to image segmentation for the first time.
SegViT (Zhang et al. 2022a) tries to perform semantic seg-
mentation with the plain vision transformer. PVT (Wang
et al. 2021) introduces the typical hierarchical architecture
in CNN into the transformer-based semantic segmentation
model. SegFormer (Xie et al. 2021) proposes an efficient
multi-scale transformer-based segmentation model.
Real-time Semantic Segmentation. Early real-time seman-
tic segmentation methods (Paszke et al. 2016; Wu, Shen,
and Hengel 2017) usually accelerate inference by compress-
ing channels or fast down-sampling. ICNet (Zhao et al.
2018a) first introduces a multi-resolution image cascade net-
work to accelerate the speed. BiSeNetV1 (Yu et al. 2018)
and BiSeNetV2 (Yu et al. 2021) adopt two-branch architec-
ture and feature fusion modules to achieve a better trade-
off between speed and accuracy. STDC (Fan et al. 2021)
rethinks the two-branch network of BiSeNet, removes the
spatial branch, and adds a detailed guidance module. DDR-
Nets (Pan et al. 2022) achieves a better trade-off by shar-
ing branches in the early stages. Very recently, some effi-
cient transformer methods for real-time segmentation have
been proposed, but they still have unresolved problems. Top-
Former (Zhang et al. 2022b) only uses transformer on 1/64
scale of the feature maps, leading to low accuracy. RT-
Former (Wang et al. 2022) and SeaFormer (Wan et al. 2023)
need frequent interaction between the two branches. This ad-
ditional computation slows down the inference speed. There
are also some single-branch and multi-branch methods. See
more discussions in appendix D.
Attention mechanism. Attention mechanism has been
widely used in computer vision in recent years. Many meth-
ods (Chu et al. 2021; Liu et al. 2021; Fang et al. 2022)
contribute to reducing the computational complexity of at-
tention mechanisms. Although some of them have achieved
linear complexity, they contain frequent shift or reshape

Figure 2: Real-time semantic segmentation paradigms.
(a) Decoupled bilateral network divides a semantic branch
and a spatial branch at the early stage. (b) Feature sharing
bilateral network separates the two branches at the latter
stage and adopts dense fusion modules. (c) Our SCTNet ap-
plies a single hierarchy branch with a semantic extraction
transformer, free from the extra branch and costly fusion
module in inference. FM: Fusion Module, SIAM: Semantic
Information Alignment Module. Dashed arrows and boxes
denote training-only.

tion for real-time segmentation (SCTNet). It can extract se-
mantic information efficiently without heavy computation
caused by the bilateral network. Specifically, SCTNet learns
long-range context from a training-only transformer seman-
tic branch to the CNN branch. To mitigate the semantic gap
between the transformer and CNN, we elaborately design
a transformer-like CNN block called CFBlock and utilize a
shared decoder head before the alignment. With the aligned
semantic information in training, the single-branch CNN
can encode the semantic information and spatial details
jointly. Therefore, SCTNet could align the semantic repre-
sentation from the large effective receptive field of trans-
former architecture while maintaining the high efficiency
of a lightweight single branch CNN architecture in infer-
ence. The overall architecture is illustrated in Figure 2(c).
Extensive experimental results on three challenging datasets
demonstrate that the proposed SCTNet has a better trade-off
between accuracy and speed than previous works. Figure 1
intuitively shows the comparison between SCTNet and other
real-time segmentation methods on the Cityscapes val set.

The main contributions of the proposed SCTNet can be

summarized as the following three aspects:

‚Ä¢ We propose a novel single-branch real-time segmentation
network called SCTNet. By learning to extract rich se-
mantic information utilizing semantic information align-
ment from the transformer to CNN, SCTNet enjoys high
accuracy of the transformer while maintaining fast in-
ference speed of the lightweight single branch CNN.

‚Ä¢ To alleviate the semantic gap between CNN features
and transformer features, we design the CFBlock (Conv-
Former Block), which could capture long-range context
as a transformer block using only convolution opera-
tions. Moreover, we propose SIAM(Semantic Informa-
tion Alignment Module) to align the features in a more

Input1/41/81/161/32FMOutput1/81/8FMFM(a)(b)(c)Input1/41/81/161/321/2FM1/41/8OutputInputOutput1/161/32SIAMSIAM1/41/81/41/81/161/32Figure 3: The architecture of SCTNet. CFBlock (Conv-Former Block, detailed in Figure 4) takes advantage of the training-
only Transformer branch (greyed-out in the dashed box) via SIAM (Semantic Information Alignment Module) which is com-
posed of BFA (Backbone Feature Alignment) and SDHA (Shared Decoder Head Alignment).

operations which bring lots of latency. MSCA (Guo et al.
2022b) shows a promising performance, but the large kernel
is not employ-friendly, and the multi-scale design of atten-
tion further incurs inference speed. External attention (Guo
et al. 2022a) has a very simple form. It uses external pa-
rameters as the key and value and implements the attention
mechanism with two linear layers. GFA(GPU-Friendly At-
tention) (Wang et al. 2022) improves external attention by
replacing head split in EA with group double norm, which
is more friendly for GPU devices.

3 Methodology

Motivation
Removing the semantic branch of bilateral networks can
significantly speed up the inference. However, this results
in shallow single-branch networks that lack long-range se-
mantic information, leading to low accuracy. While using
deep encoders and powerful decoders or complex enhance-
ment modules can recover accuracy, it slows down the infer-
ence process. To address this issue, we propose a training-
only alignment method that enriches semantic information
without sacrificing inference speed. Specifically, we pro-
posed SCTNet, a single-branch convolution network with a
training-only semantic extraction transformer, which owns
high accuracy of transformer and fast inference speed of
CNN. The overview of SCTNet is presented in Figure 3.

Conv-Former Block
As different types of networks, the feature representations
extracted by CNN and transformer significantly differ. Di-
rectly aligning the features between the CNN and the trans-
former makes the learning process difficult, resulting in lim-

ited performance improvement. In order to make the CNN
branch easily learns how to extract high-quality semantic in-
formation from the transformer branch, we design the Conv-
Former Block. Conv-Former Block simulates the structure
of the transformer block as much as possible to learn the se-
mantic information of the transformer branch better. Mean-
while, the Conv-Former Block implements the attention
function using only efficient convolution operations.

The structure of the Conv-Former Block is similar to the
structure of a typical transformer encoder (Vaswani et al.
2017), as presented in the left of Figure 4. The process can
be described as follows:

f = N orm(x + ConvAttention(x)),
y = N orm(f + F F N (f )),

(1)

where N orm(¬∑) refers to batch normalization (Ioffe and
Szegedy 2015), and x, f , y denote input, hidden feature and
output, respectively.
Convolutional Attention. Attention mechanisms used for
real-time segmentation should have the property of low la-
tency and powerful semantic extraction ability. As discussed
in the related work, We believe GFA is a potential candidate.
Our convolutional attention is derived from GFA.

There are two main differences between GFA and the pro-
posed convolutional attention. Firstly, we replace the matrix
multiplication in GFA with pixel-wise convolution opera-
tions. Point convolution is equivalent to pixel-to-pixel multi-
plication but without feature flattening and reshaping opera-
tions. These operations are detrimental to maintaining the
inherent spatial structure and bring in extra inference la-
tency. Moreover, convolution provides a more flexible way
to extend external parameters. Then, due to the semantic
gap between the transformer and CNN, it is not enough to

Backbone Feature AlignmentSIAMSharedDecoderHeadAlignmentmDecoderStemConv BlockConv BlockConv BlockTransformer BlockStemTransformer BlockTransformer BlockTransformer BlockCF BlockTraining Onlyùêªùêª4√óùëäùëä4√óCConv BlockCF BlockCF BlockDecoderConv Blockùêªùêª8√óùëäùëä8√ó2Cùêªùêª16√óùëäùëä16√ó4Cùêªùêª32√óùëäùëä32√ó8Cùêªùêªùëêùëê√óùëäùëäùëêùëê√óùê∂ùê∂ùëêùëêùêªùêª√óùëäùëä√óùê∂ùê∂ùë°ùë°ùêªùêª√óùëäùëä√óùê∂ùê∂ùë°ùë°Resize&Projection‚Ä¶ReshapeBFASemantic Alignment LossProjectionDecoderDecoderDecoderSemantic Alignment LossSDHABackbone Feature Alignment. Thanks to the transformer-
like architecture of the Conv-Former Block, the alignment
loss can easily align the Conv-Former Block‚Äôs features with
the features of transformers. In short, the backbone feature
alignment first down-sample or up-sample the feature from
the transformer and CNN branches for alignment. Then it
projects the feature of the CNN to the dimension of the trans-
former. The projection can: 1) unify the number of channels
and 2) avoid direct alignment of features, which damages
the supervision of ground truth for the CNN in the training
process. Finally, a semantic alignment loss is applied to the
projected features to align the semantic representations.
Shared Decoder Head Alignment. Transformer decoders
often use the features of multiple stages for complex de-
coding, while SCTNet decoder only picks the features of
stage2&stage4. Considering the significant difference in de-
coding space between them, direct alignment of the decod-
ing features and output logits can only get limited improve-
ment. Therefore, we propose shared decoder head align-
ment. Specifically, the concatenation stage2&stage4 features
of the single-branch CNN are input into a point convolution
to expand the dimension. Then the high-dimension features
are passed through the transformer decoder. The transformer
decoder‚Äôs new output features and logits are used to calcu-
late alignment loss with the origin outputs of the transformer
decoder.

Overall Architecture
To reduce computational costs while obtain rich semantic
information, we simplify the popular two-branches archi-
tecture to one swift CNN branch for inference and a trans-
former branch for semantic alignment only for training.
Backbone. To improve the inference speed, SCTNet adopts
a typical hierarchical CNN backbone. SCTNet starts from
a stem block consisting of two sequential 3√ó3 convolution
layers. The former two stages consist of stacked residual
blocks (He et al. 2016), and the latter two stages include
the proposed transformer-like blocks called Conv-Former
Blocks (CFBlocks). The CFBlock employs several elabo-
rately designed convolution operations to perform the simi-
lar long-range context capturing function of the transformer
block. We apply a convdown layer consisting of a strid-
den convolution with batch normal and ReLu activation for
down-sampling at the beginning of stage 2 ‚àº 4, which is
omitted in Figure 3 for clarity.
Decoder Head. The decoder head consists of
a
DAPPM (Pan et al. 2022) and a segmentation head.
To further enrich the context information, we add a DAPPM
after the output of stage 4. Then we concatenate the output
with the feature map of Stage 2. Finally, this output feature
is passed into a segmentation head. Precisely, the segmen-
tation head consists of a 3√ó3 Conv-BN-ReLU operator
followed by a 1√ó1 convolution classifier.
Training Phase. It is well known that transformer excels
at capturing global semantic context. On the other hand,
CNN has been widely proven to be better at modeling hierar-
chical locality information than transformers. Motivated by
the advantages of transformer and CNN, we explore equip-
ping a real-time segmentation network with both merits. We

Figure 4: Design of Conv-Former Block (left) and the de-
tails of convolutional attention (right). GDN means Grouped
Double Normalization. ‚äó means convolution operations, ‚äï
stands for addition, and k means the kernel size.

capture rich context that simply calculates the similarity be-
tween several learnable vectors and each pixel and then en-
hances the pixels according to the similarity map and the
learnable vectors. To better align the semantic information
of the transformer, we enlarge the learnable vectors to learn-
able kernels. On the one hand, this converts the similarity
calculation between pixel and learnable vectors to that be-
tween pixel patches with learnable kernels. On the other
hand, the convolution operation with learnable kernels re-
tains more local spatial information to some extent.

X = Œ∏ (X ‚äó K) ‚äó K T ,

(2)

where X ‚àà RC√óH√óW ,K ‚àà RC√óN √ók√ók, K T ‚àà
RN √óC√ók√ók represents input image and learnable query and
key, respectively. C, H, W denote the channel, height, and
width of the feature map, respectively. N denotes the num-
ber of learnable parameters, and k denotes the kernel size of
the learnable parameters. Œ∏ symbolizes the grouped double
normalization, which applies softmax on the dimension of
H √ó W and grouped L2 Norm on the dimension of N . ‚äó
means convolution operations.

Taking efficiency into consideration, we implement the
convolution attention with stripe convolution rather than
standard convolutions. More specifically, we utilize a 1 √ó k
and a k √ó 1 convolution to approximate a k √ó k convolu-
tion layer. Figure 4 illustrate the implementation details of
convolution attention.
Feed Forward Network. Typical FFN plays a vital role in
providing position encoding and embedding channels. The
typical FFN (Feed Forward Network) in recent transformer
models consists of a expand point convolution, a depth-wise
3√ó3 convolution, and a squeeze point convolution. Different
from typical FFN, our FFN is made up of two standard 3 √ó
3 convolution layers. Compared with the typical FFN, our
FFN is more efficient and provides a larger receptive field.

Semantic Information Alignment Module
A simple yet effective alignment module is proposed to con-
duct the feature learning in the training process, as shown in
Figure 3. It can be divided into backbone feature alignment
and shared decoder head alignment.

Convolution AttentionAdd & NormFFNAdd & NormInputOutputGDNHWC‚Ä¶CkN‚Ä¶CkNHWC‚Ä¶CNkCkN‚Ä¶GDNùë≤ùíÜùíöùüèùë≤ùíÜùíöùüêùëΩùíÇùíçùíñùíÜùüêùëΩùíÇùíçùíñùíÜùüèùë∏ùíñùíÜùíìùíöMethod
SFNet-ResNet18

Reference
ECCV 2020
AFFormer-B-Seg100 AAAI 2023
AAAI 2023
AFFormer-B-Seg75
AAAI 2023
AFFormer-B-Seg50
NeurIPS 2022b
SegNext-T-Seg100
NeurIPS 2022b
SegNext-T-Seg75
CVPR 2023
PIDNet-S
CVPR 2023
PIDNet-M
CNN-based Bilateral Networks

BiSeNet-ResNet18
BiSeNetV2-L
STDC1-Seg75
STDC2-Seg75
STDC1-Seg50
STDC2-Seg50
DDRNet-23-S
DDRNet-23

ECCV 2018
IJCV 2021
CVPR 2021
CVPR 2021
CVPR 2021
CVPR 2021
TIP 2022
TIP 2022

Transformer-based Bilateral Networks
TopFormer-B-Seg100 CVPR 2022b
CVPR 2022b
TopFormer-B-Seg50
ICLR 2023
SeaFormer-B-Seg100
ICLR 2023
SeaFormer-B-Seg50
NeurIPS 2022
RTFormer-S
NeurIPS 2022
RTFormer-B
SCTNet-S-Seg50
Ours
SCTNet-S-Seg75
Ours
SCTNet-B-Seg50
Ours
SCTNet-B-Seg75
Ours
SCTNet-B-Seg100
Ours

#Params‚Üì
12.3M
3.0M
3.0M
3.0M
4.3M
4.3M
7.6M
34.4M

49.0M
-
14.2M
22.2M
14.2M
22.2M
5.7M
20.1M

5.1M
5.1M
8.6M
8.6M
4.8M
16.8M
4.6M
4.6M
17.4M
17.4M
17.4M

Resolution
2048 √ó 1024
2048 √ó 1024
1536 √ó 768
1024 √ó 512
2048 √ó 1024
1536 √ó 768
2048 √ó 1024
2048 √ó 1024

1536 √ó 768
1024 √ó 512
1536 √ó 768
1536 √ó 768
1024 √ó 512
1024 √ó 512
2048 √ó 1024
2048 √ó 1024

2048 √ó 1024
1024 √ó 512
2048 √ó 1024
1024 √ó 512
2048 √ó 1024
2048 √ó 1024
1024 √ó 512
1536 √ó 768
1024 √ó 512
1536 √ó 768
2048 √ó 1024

FPS(TRT)‚Üë
50.5
58.3
96.4
148.4
46.5
78.3
127.1
90.7

182.9
102.3
209.5
149.2
397.6
279.7
138.9
101.9

128.4
410.9
103.6
231.6
-
-
451.2
233.3
374.6
186.6
105.0

FPS(Torch)‚Üë mIoU(%)‚Üë

24.0
28.4
38.6
49.5
28.1
45.6
93.2
39.8

112.3
67.6
101.9
84.3
146.2
94.6
106.7
56.7

81.4
95.7
37.5
45.2
89.6
50.2
160.3
149.2
144.9
105.2
62.8

79.0
78.7
76.5
73.5
79.8
78.0
78.8
80.1

74.8
75.8
74.5
77.0
72.2
74.2
77.8
79.5

76.3
70.7
77.7
72.2
76.3
79.3
72.8
76.1
76.5
79.8
80.5

Table 1: Comparisons with other state-of-the-art real-time methods on Cityscapes val set. Seg100, Seg75, Seg50 denote
the input size of 1024 √ó 2048, 768 √ó 1536, 512 √ó 1024, respectively. #Params refers to the number of parameters.

propose a single-branch CNN that learns to align its fea-
tures with those of a powerful transformer, which is illus-
trated in the blue dotted box in Figure 3. This feature align-
ment enables the single-branch CNN to extract both rich
global context and detailed spatial information. Specifically,
there are two streams in the training phase. SCTNet adopts
a train-only transformer as the semantic branch to extract
powerful global semantic context. The semantic information
alignment module supervises the convolution branch to align
high-quality global context from the transformer.
Inference Phase. To avoid the sizeable computation costs
of two branches, only the CNN branch is deployed in the in-
ference phase. With the transformer-aligned semantic infor-
mation, the single-branch CNN can generate accurate seg-
mentation results without the extra semantic or costly dense
fusion. To be more specific, the input image is fed into a
single-branch hierarchy convolution backbone. Then the de-
coder head picks up the features in the backbone and con-
ducts simple concatenation followed by pixel-wise classifi-
cation.

Alignment Loss.
For better alignment of semantic information, a alignment
loss focusing on semantic information rather than spatial in-
formation is needed. In the implementation, we use CWD

Loss (channel-wise distillation loss) (Shu et al. 2021) as the
alignment loss, which shows better results than other loss
functions. CWD Loss can be summarized as follows:

œï(xc) =

exp( xc,i
T )
i=1 exp( xc,i
T )

(cid:80)W ¬∑H

,

Lcwd =

T 2
C

C
(cid:88)

H¬∑W
(cid:88)

c=1

i=1

œï(xc,i

T ) ¬∑ log

(cid:104) œï(xc,i
T )
œï(xc,i
S )

(cid:105)

,

(3)

(4)

where c = 1, 2, ..., C indexes the channel, and i =
1, 2, ..., H ¬∑ W denotes the spatial location, xT and xS are
the feature maps of the transformer branch and CNN branch,
respectively. œï converts the feature activation into a channel-
wise probability distribution, removing the influences of
scales between the transformer and the compact CNN. To
minimize Lcwd, œï(xc,i
T ) is
large. But when œï(xc,i
S ) does
not matter. This force the CNN to learn the distribution of the
foreground salience, which contains the semantic informa-
tion. T denotes a hyper-parameter called temperature. And
the larger T is, the softer the probability distribution is.

S ) should be large when œï(xc,i
T ) is small, the value of œï(xc,i

For more discussions and detailed ablation studies on the

choices of alignment loss, please refer to appendix B.

4 Experiments

Datasets and Implementation Details
We conduct experiments of the SCTNet on three datasets,
i.e., Cityscapes (Cordts et al. 2016), ADE20K (Zhou et al.
2017), and COCO-Stuff-10K (Caesar, Uijlings, and Ferrari
2018) to demonstrate the effectiveness of our method. For a
fair comparison, we build our base model SCTNet-B with
a comparable size to RTFormer-B/DDRNet-23/STDC2.
Furthermore, we also introduce a smaller variant called
SCTNet-S. We first pre-train our CNN backbones on Ima-
geNet (Deng et al. 2009), then fine-tune it on semantic seg-
mentation datasets. The semantic transformer branch in the
training phrase can be any hierarchical transformer network.
In our implementation, we choose SegFormer as the trans-
former branch for all experiments. We measure the infer-
ence speed of all methods on a single NVIDIA RTX 3090.
All reported FPS results are obtained under the same input
resolution for fair performance comparison unless specified.
For Cityscapes, we measure the speed implemented with
both torch and tensor-RT. More details on model instanti-
ation, metrics, and training settings of specific datasets can
be found in appendix A.

Comparison with State-of-the-art Methods
Results on Cityscapes. The corresponding results on
Cityscapes(Cordts et al. 2016) are shown in Table 1. Our
SCTNet outperforms other real-time methods by a large
margin and attains the best speed-accuracy trade-off with
both tensorRT and Torch implementations. For example,
our SCTNet-B-Seg100 achieves 80.5% mIoU at 62.8 FPS,
which is a new state-of-the-art performance for real-time
segmentation. Our SCTNet-B-Seg75 reaches 79.8% mIoU,
which is better than the state-of-the-art transformer-based
bilateral network RTFormer-B and cnn-based bilateral net-
work DDRNet-23 in accuracy but has a two times faster
speed. Our SCTNet-B is faster at all input resolutions with
better mIoU results than all other methods. Besides, our
SCTNet-S also achieves a better trade-off compared with
STDC2 (Fan et al. 2021), RTFormer-S (Wang et al. 2022),
SeaFormer-B (Wan et al. 2023) and TopFormer-B (Zhang
et al. 2022b). The origin speed with specific devices of
more methods and the comparison of FPS on NVIDIA RTX
2080Ti GPU can be found in appendix C.
Results on ADE20K. On ADE20K(Zhou et al. 2017),
our SCTNet achieves the best accuracy with the fastest
speed. For instance, our SCTNet-B achieves 43.0% mIoU
at superior 145.1 FPS, which is about 1.6 times faster
than RTFormer-B (Wang et al. 2022) with 0.9% higher
mIoU performance. Our SCTNet-S reaches 37.7% mIoU
while keeping the highest FPS among all other methods on
ADE20K (Zhou et al. 2017). Considering the large variety of
images and various semantic categories in ADE20K (Zhou
et al. 2017), this outstanding results further also demonstrate
the generalization capability of our SCTNet.
Results on COCO-Stuff-10K. The corresponding results
on COCO-Stuff-10K are shown in Table 3. SCTNet shows
SOTA performance and maintains the highest inference
speed on COCO-Stuff-10K in real-time semantic segmen-

Method
FCN(MV2)
PSPNet(MV2)
DeepLabV3+(MV2)
SegFormerB0
TopFormer-B
SeaFormer-B
SegNext-T
AFFormer-B
RTFormer-S
RTFormer-B
SCTNet-S
SCTNet-B

#Params‚Üì
9.8M
13.7M
15.4M
3.8M
5.1M
8.6M
4.3M
3.0M
4.8M
16.8M
4.7M
17.4M

FPS‚Üë mIoU(%)‚Üë
64.4‚àó
57.7‚àó
43.1‚àó
84.4
96.2
44.5
60.3
49.6
95.2
93.4
158.4
145.1

19.7
29.6
34.0
37.4
39.2
41.0
41.1
41.8
36.7
42.1
37.7
43.0

Table 2: Comparisons with other state-of-the-art real-
time methods on ADE20K. The FPS is measured at res-
olution 512 √ó 512. * means speed from other papers, MV2
stands for MobileNetV2.

Method
PSPNet50
ICNet
BiSeNetV2-L
TopFormer-B
SeaFormer-B
AFFormer-B
DDRNet23
RTFormer-B
SCTNet-B

#Params‚Üì
-
-
-
5.1M
8.6M
3.0M
20.1M
16.8M
17.4M

FPS‚Üë mIoU(%)‚Üë
6.6‚àó
35.7‚àó
65.1
94.7
41.9
46.5
108.8
90.9
141.5

32.6
29.1
28.7
33.4
34.1
35.1
32.1
35.3
35.9

Table 3: Comparisons with other state-of-the-art real-
time methods on COCO-Stuff-10K test set. The FPS is
measured at resolution 640 √ó 640.

tation methods. With the input size 640 √ó 640, SCTNet-B
achieves 35.9% mIoU at 141.5 FPS, which is 0.6% higher
than RTFormer-B, and about 1.6 times faster. The com-
prehensive accuracy trade-off comparison on ADE20K and
COCO-Stuff-10K can be found in Figure 6 and 7 (Appendix
C) respectively.

Ablation Study
Comparison on Different Types of Blocks. To verify the
effectiveness of our proposed CFBlock, we replace the CF-
Blocks with other kinds of convolution blocks and trans-
former blocks in real-time segmentation. For quick evalu-
ations, all these results in Table 4 are not pre-trained on Im-
ageNet. We select four kinds of blocks for comparison. As
shown in Table 4, our CFBlock outperforms the typical Res-
Block and the lightweight SegFormer Block by a significant
mIoU margin. Moreover, compared with the state-of-the-
art GFABlock (Wang et al. 2022) and MSCANBlock from
SegNext (Guo et al. 2022b), our CFBlock get better speed
and accuracy trade-off. Our CFBlock has 0.9% higher mIoU
than GFABlock and maintains the similar performance with
fewer parameters and faster speed than MSCANBlock. This
also demonstrates that our SCTNet can better mitigate the
gap of semantic information between CNN and transformer
while getting rid of the high computation cost.

(a) Image

(b) GT

(c) DDRNet-23

(d) RTFormer-B

(e) SCTNet-B

Figure 5: Visualization results on Cityscapes validation set. Compared with SeaFormer-B(Wan et al. 2023) and STDC2 (Fan
et al. 2021), SCTNet-B generates masks with finer details as highlighted in the light blue box and more accurate large-area
predictions, as highlighted in the yellow box.

FPS‚Üë mIoU(%)‚Üë param
Block
15.3M
77.9
ResBlock
66.7
22.2M
77.7
SegFormerBlock 57.3
16.3M
78.5
GFABlock
66.2
19.8M
79.3
60.5
MSCANBlock
79.4
CFBlock (Ours) 62.8
17.4M

Table 4: Comparison of different blocks.

Effectiveness of the Semantic Information Alignment
Module. Although our SIAM(semantic information align-
ment module) is closely related to the elaborately designed
SCTNet, it can also improve the performance of other CNN
and transformer segmentation methods. As presented in Ta-
ble 5, employing our SIAM attains consistent improvements
on SegFormer, SegNext, SeaFormer, and DDRNet, which
proves the effectiveness and generalization capability of our
proposed SIAM. At the same time, as representatives of the
bilateral-branch transformer and the bilateral-branch CNN
network, the improvements of SeaFormer and DDRNet are
relatively slim. This may be attributed to the fact that their
bilateral-branch network structure already benefits from the
additional semantic branch. And this also confirms that the
cooperation of our SIMA and training-only transformer does
play the role of the semantic branch in the bilateral-branch
network, leading to improvements in the accuracy of the
single-branch network.
Components Ablation. We explore the effect of the pro-
posed components in Table 6. Take Seg100 as an example,
simply replacing the Resblock with our CFBlock brings a
2.1% improvement of mIoU with little speed loss. The BFA
leads to a 1.2% higher mIoU, and the SDHA further attains
a 0.8% improvement of mIoU without sacrificing speed.
More Ablation Studies. For more detailed ablation studies
on the design of CFBlock, the discussion of semantic align-
ment loss, and the choice of semantic transformer branch in
training phrase, please refer to the appendix B.

Seg100(%)
79.8
80.1(+0.3)
74.7

Block
SegNext-T
SegNext-T+SIAM
SegFormer-B0
SegFormer-B0+SIAM 77.3(+2.6)
SeaFormer-B
SeaFormer-B+SIAM
DDRNet-23
DDRNet-23+SIAM
SCTNet-B-SIAM
SCTNet-B (Ours)

77.7
78.1(+0.4)
79.5
79.6(+0.1)
78.5
80.5(+2.0)

Seg75(%)
78.0
78.2(+0.2)
74.4
76.8(+2.4)
-
-
-
-
77.5
79.8(+2.3)

Seg50(%)
-
-
70.7
72.5(+1.8)
72.2
72.5(+0.3)
-
-
75.2
76.5(+1.3)

Table 5: Comparison of the effect of the SIAM.

Components
Baseline
+CFBlock
+BFA‚àó
+SDHA

Seg100(%)
76.4
78.5(+2.1)
79.7(+1.2)
80.5(+0.8)

Seg75(%)
76.0
77.5(+1.5)
79.1(+1.6)
79.8(+0.7)

Seg50(%)
73.0
75.2(+2.2)
75.7(+0.5)
76.5(+0.8)

FPS(Seg100)
66.7
62.8
62.8
62.8

Table 6: Ablation studies on the components of SCTNet

Visualization Results
Figure 5 shows visualization results on Cityscapes (Cordts
et al. 2016) validation set. Compared with DDRNet and RT-
Former, our SCTNet provides not only better results for
those classes with large areas like roads, sidewalks, and
big trucks but also more accurate boundaries for small or
thin objects such as poles, traffic lights, traffic signs, and
cars. This indicates that SCTNet extracts high-quality long-
range context while preserving fine details. More visualiza-
tion results on Cityscapes and ADE20K can be found in ap-
pendix E.

5 Conclusion
In this paper, we propose SCTNet, a novel single-branch
architecture that can extract high-quality long-range con-
text without extra inference computation cost. Extensive ex-
periments demonstrate that SCTNet achieves new state-of-
the-art results. Moreover, by demonstrating the efficiency of

SCTNet, we provide a novel insight for the semantic branch
in the bilateral-branch network and a new way to boost the
real-time segmentation community by not only adopting the
structure of the transformer but also unitizing its knowledge.

6 Acknowledgments

This work was supported by Hubei Provincial Natural Sci-
ence Foundation of China No.2022CFA055 and the National
Natural Science Foundation of China No.62176097.

References
Ahn, S.; Hu, S. X.; Damianou, A.; Lawrence, N. D.; and Dai,
Z. 2019. Variational information distillation for knowledge
In Proceedings of the IEEE/CVF conference on
transfer.
computer vision and pattern recognition, 9163‚Äì9171.

Bo, D.; Pichao, W.; and Wang, F. 2023. AFFormer:
Head-Free Lightweight Semantic Segmentation with Linear
Transformer. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence.

Caesar, H.; Uijlings, J.; and Ferrari, V. 2018. Coco-stuff:
In Proceedings of the
Thing and stuff classes in context.
IEEE conference on computer vision and pattern recogni-
tion, 1209‚Äì1218.

Chen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and
Yuille, A. L. 2014. Semantic image segmentation with deep
convolutional nets and fully connected crfs. arXiv preprint
arXiv:1412.7062.

Chen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and
Yuille, A. L. 2017. Deeplab: Semantic image segmentation
with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 40(4): 834‚Äì848.

Chen, L.-C.; Zhu, Y.; Papandreou, G.; Schroff, F.; and
Adam, H. 2018. Encoder-decoder with atrous separable
convolution for semantic image segmentation. In European
Conference on Computer Vision, 801‚Äì818.

Chu, X.; Tian, Z.; Wang, Y.; Zhang, B.; Ren, H.; Wei, X.;
Xia, H.; and Shen, C. 2021. Twins: Revisiting the design of
spatial attention in vision transformers. Advances in Neural
Information Processing Systems, 34: 9355‚Äì9366.

Cordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,
M.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.
2016. The cityscapes dataset for semantic urban scene un-
derstanding. In IEEE Conference on Computer Vision and
Pattern Recognition, 3213‚Äì3223.

Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-
Fei, L. 2009.
Imagenet: A large-scale hierarchical image
database. In IEEE Conference on Computer Vision and Pat-
tern Recognition, 248‚Äì255. Ieee.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929.

Distill-
arXiv preprint

Fan, M.; Lai, S.; Huang, J.; Wei, X.; Chai, Z.; Luo, J.; and
Wei, X. 2021. Rethinking bisenet for real-time semantic seg-
In IEEE/CVF Conference on Computer Vision
mentation.
and Pattern Recognition, 9716‚Äì9725.
Fang, J.; Xie, L.; Wang, X.; Zhang, X.; Liu, W.; and Tian,
Q. 2022. Msg-transformer: Exchanging local spatial infor-
In IEEE/CVF
mation by manipulating messenger tokens.
Conference on Computer Vision and Pattern Recognition,
12063‚Äì12072.
Fu, J.; Liu, J.; Tian, H.; Li, Y.; Bao, Y.; Fang, Z.; and Lu,
H. 2019. Dual attention network for scene segmentation.
In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 3146‚Äì3154.
Guo, M.-H.; Liu, Z.-N.; Mu, T.-J.; and Hu, S.-M. 2022a. Be-
yond self-attention: External attention using two linear lay-
ers for visual tasks. IEEE Transactions on Pattern Analysis
and Machine Intelligence.
Guo, M.-H.; Lu, C.-Z.; Hou, Q.; Liu, Z.; Cheng, M.-M.; and
Hu, S.-M. 2022b. Segnext: Rethinking convolutional atten-
tion design for semantic segmentation. Advances in Neural
Information Processing Systems, 35: 1140‚Äì1156.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 770‚Äì778.
Hinton, G.; Vinyals, O.; and Dean, J. 2015.
ing the knowledge in a neural network.
arXiv:1503.02531.
Huang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y.; and
Liu, W. 2019. Ccnet: Criss-cross attention for semantic seg-
mentation. In IEEE International Conference on Computer
Vision, 603‚Äì612.
Ioffe, S.; and Szegedy, C. 2015. Batch normalization: Accel-
erating deep network training by reducing internal covariate
In International Conference on Machine Learning,
shift.
448‚Äì456. pmlr.
Li, K.; Yu, R.; Wang, Z.; Yuan, L.; Song, G.; and Chen, J.
2022a. Locality guidance for improving vision transform-
ers on tiny datasets. In European Conference on Computer
Vision, 110‚Äì127. Springer.
Li, X.; You, A.; Zhu, Z.; Zhao, H.; Yang, M.; Yang, K.; Tan,
S.; and Tong, Y. 2020. Semantic flow for fast and accurate
scene parsing. In European Conference on Computer Vision,
775‚Äì793. Springer.
Li, X.; Zhang, J.; Yang, Y.; Cheng, G.; Yang, K.; Tong, Y.;
and Tao, D. 2022b. Sfnet: Faster, accurate, and domain
agnostic semantic segmentation via semantic flow. arXiv
preprint arXiv:2207.04415.
Lin, G.; Milan, A.; Shen, C.; and Reid, I. 2017. Refinenet:
Multi-path refinement networks for high-resolution seman-
In IEEE/CVF Conference on Computer
tic segmentation.
Vision and Pattern Recognition, 1925‚Äì1934.
Liu, Y.; Cao, J.; Li, B.; Hu, W.; Ding, J.; and Li, L. 2022.
Cross-Architecture Knowledge Distillation. In Asian Con-
ference on Computer Vision, 3396‚Äì3411.
Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin,
S.; and Guo, B. 2021. Swin transformer: Hierarchical vision

Xu, J.; Xiong, Z.; and Bhattacharyya, S. P. 2023. PIDNet:
A Real-Time Semantic Segmentation Network Inspired by
PID Controllers. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, 19529‚Äì
19539.
Yu, C.; Gao, C.; Wang, J.; Yu, G.; Shen, C.; and Sang, N.
2021. Bisenet v2: Bilateral network with guided aggregation
for real-time semantic segmentation. International Journal
of Computer Vision, 129: 3051‚Äì3068.
Yu, C.; Wang, J.; Peng, C.; Gao, C.; Yu, G.; and Sang, N.
2018. BiSeNet: Bilateral segmentation network for real-
In European Conference on
time semantic segmentation.
Computer Vision, 325‚Äì341.
Yuan, Y.; Huang, L.; Guo, J.; Zhang, C.; Chen, X.; and
Wang, J. 2018. Ocnet: Object context network for scene
parsing. arXiv preprint arXiv:1809.00916.
Zhang, B.; Tian, Z.; Tang, Q.; Chu, X.; Wei, X.; Shen, C.;
et al. 2022a. Segvit: Semantic segmentation with plain vi-
sion transformers. Advances in Neural Information Process-
ing Systems, 35: 4971‚Äì4982.
Zhang, W.; Huang, Z.; Luo, G.; Chen, T.; Wang, X.; Liu,
W.; Yu, G.; and Shen, C. 2022b. TopFormer: Token pyra-
mid transformer for mobile semantic segmentation.
In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 12083‚Äì12093.
Zhao, H.; Qi, X.; Shen, X.; Shi, J.; and Jia, J. 2018a. Icnet for
real-time semantic segmentation on high-resolution images.
In European Conference on Computer Vision, 405‚Äì420.
Zhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyramid
scene parsing network. In IEEE Conference on Computer
Vision and Pattern Recognition, 2881‚Äì2890.
Zhao, H.; Zhang, Y.; Liu, S.; Shi, J.; Loy, C. C.; Lin, D.; and
Jia, J. 2018b. Psanet: Point-wise spatial attention network
In European Conference on Computer
for scene parsing.
Vision, 267‚Äì283.
Zheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y.; Fu,
Y.; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-
mantic segmentation from a sequence-to-sequence perspec-
tive with transformers. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, 6881‚Äì6890.
Zhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; and
Torralba, A. 2017. Scene parsing through ade20k dataset. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, 633‚Äì641.
Zhu, J.; Luo, Y.; Zheng, X.; Wang, H.; and Wang, L.
2023. A Good Student is Cooperative and Reliable: CNN-
Transformer Collaborative Learning for Semantic Segmen-
tation. arXiv preprint arXiv:2307.12574.

transformer using shifted windows. In IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, 10012‚Äì
10022.
Long, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convo-
lutional networks for semantic segmentation. In IEEE Con-
ference on Computer Vision and Pattern Recognition, 3431‚Äì
3440.
Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101.
Milletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-Net:
Fully convolutional neural networks for volumetric medical
image segmentation. In International Conference on 3D Vi-
sion.
Pan, H.; Hong, Y.; Sun, W.; and Jia, Y. 2022. Deep dual-
resolution networks for real-time and accurate semantic seg-
mentation of traffic scenes. IEEE Transactions on Intelligent
Transportation Systems, 24(3): 3448‚Äì3460.
Paszke, A.; Chaurasia, A.; Kim, S.; and Culurciello, E. 2016.
Enet: A deep neural network architecture for real-time se-
mantic segmentation. arXiv preprint arXiv:1606.02147.
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-
volutional networks for biomedical image segmentation. In
International Conference on Medical Image Computing and
Computer-Assisted Intervention, 234‚Äì241. Springer.
Shu, C.; Liu, Y.; Gao, J.; Yan, Z.; and Shen, C. 2021.
Channel-wise knowledge distillation for dense prediction.
In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 5311‚Äì5320.
Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,
A.; and J¬¥egou, H. 2021. Training data-efficient image trans-
In International
formers & distillation through attention.
Conference on Machine Learning, 10347‚Äì10357. PMLR.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-
tention is all you need. Neural Information Processing Sys-
tems, 30.
Wan, Q.; Huang, Z.; Lu, J.; Yu, G.; and Zhang, L.
2023. SeaFormer: Squeeze-enhanced Axial Transformer
arXiv preprint
for Mobile Semantic Segmentation.
arXiv:2301.13156.
Wang, J.; Gou, C.; Wu, Q.; Feng, H.; Han, J.; Ding, E.; and
Wang, J. 2022. RTFormer: Efficient Design for Real-Time
Semantic Segmentation with Transformer. In Advances in
Neural Information Processing Systems.
Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;
Lu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-
former: A versatile backbone for dense prediction without
convolutions. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 568‚Äì578.
Wu, Z.; Shen, C.; and Hengel, A. v. d. 2017. Real-time
arXiv
semantic image segmentation via spatial sparsity.
preprint arXiv:1712.00213.
Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;
and Luo, P. 2021. SegFormer: Simple and efficient design
for semantic segmentation with transformers. Advances in
Neural Information Processing Systems, 34: 12077‚Äì12090.

Appendix
The structure of this supplementary material can be sum-
marized as follows. Section A provides more implementa-
tion details. Section B shows more detailed ablation stud-
ies on the design of CFBlock, semantic alignment loss, and
semantic transformer branch in training phrase. Section C
presents the comparison on NVIDIA RTX 2080Ti and orig-
inal speed with specific devices of more methods. Section D
gives some discussions about the proposed SCTNet. More-
over, more visualization results are displayed in Section E.

A More Implementation Details

ImageNet Pre-training
For a fair comparison, the CNN backbone of the proposed
SCTNet is pre-trained on ImageNet-1K (Deng et al. 2009)
using 8 GPUs. In the pre-training phase, we build our code
based on the MMClassification, following the training set-
tings of swin-transformer (Liu et al. 2021) on ImageNet-1K.
The main details of the pre-training settings are presented in
Table 7.

Table 7: Training settings on ImageNet-1K.

config
optimizer
base learning rate
weight decay
optimizer momentum
learning rate schedule
minimum learning rate
warmup epochs
warmup learning rate
training epochs
batch size
augmentation
random resized crop
random flip
mixup
cutmix
random erasing

value
AdamW
0.001
0.05
Œ≤1, Œ≤2=0.9, 0.999
cosine decay
1e-5
20
1e-6
300
1024
RandAug(9, 0.5)
224
0.5
0.8
1.0
0.25

Datasets and Implementation Details
Cityscapes. Cityscapes (Cordts et al. 2016) is an urban street
semantic scene parsing dataset from a car‚Äôs perspective. It
contains 5,000 fine annotated images with 19 categories for
segmentation, which are split into training, validation, and
test sets, with 2,975, 500, and 1,525 images, respectively.
For Cityscapes (Cordts et al. 2016), we train all models us-
ing AdamW (Loshchilov and Hutter 2017) optimizer with
the initial learning rate of 0.0004 and the weight decay of
0.0125. We apply the poly learning policy with the power
of 0.9 to drop the learning rate and implement the data
augmentation method, including random cropping, random
scaling, and random horizontal flipping. For Seg75& Seg50,
we adopt random cropping into 1024√ó512 and random scal-
ing in the range of 0.25 to 1.5, while 1024 √ó 1024 and from

Table 8: Ablation studies on the kernel size of the learn-
able kernels in CFBlock.

FPS‚Üë mIoU(%)‚Üë
Kernel Size
1 √ó 1
67.4
1 √ó 1 + 1 √ó 1 65.9
1 √ó 3 + 3 √ó 1 64.8
1 √ó 5 + 5 √ó 1 63.3
1 √ó 7 + 7 √ó 1 62.8
1 √ó 9 + 9 √ó 1 61.9
7 √ó 7
59.8

78.4
78.7
79.0
79.1
79.4
79.3
78.8

0.5 to 2.0 for Seg100. All models are trained with 160k iter-
ations, a batch size of 16. For the Tensor-RT speed, we apply
TensorRT v8.2.3 for acceleration.
ADE20K. ADE20K (Zhou et al. 2017) is a scene parsing
dataset with 150 fine-grained semantic classes, which is split
into 20K, 2K, and 3K images for training, validation, and
testing, respectively. For ADE20K, all models are trained
with a batch size of 32 for 160k iterations. Except for the
cropping size of 512 √ó 512 and the initial learning rate of
0.0005, all other training settings are identical to the training
settings of Seg100 for Cityscapes (Cordts et al. 2016).
COCO-Stuff. COCO-Stuff (Caesar, Uijlings, and Ferrari
2018) . COCO-Stuff contains 171 semantic classes and
10K(9K for training and 1K for testing) difficult sam-
ples with pixel-level stuff annotations that are collected in
COCO. We use AdamW optimizer with weight decay of
0.00006 and an initial learning rate of 0.01. Data augmen-
tation includes random cropping into 640 √ó 640 and ran-
dom scaling in the range of 0.5 to 2.0. All other training
details are the same as the training settings of Seg100 for
Cityscapes (Cordts et al. 2016).
Metric. We adopt mIoU (mean intersection over union),
FPS (Frames Per Second), and parameters as the evaluation
metrics. mIoU is a widely used evaluation metric in seman-
tic segmentation tasks. It quantifies the degree of overlap
between the predicted segmentation and the ground truth
segmentation by computing the ratio of the intersection to
the union of the two sets of pixels. The mIoU score is ob-
tained by averaging the IoU scores across all classes in the
dataset. It provides a measure of the overall accuracy of the
segmentation model. Frames Per Second (FPS) is a measure
of the number of frames that the network can process per
second, which is affected by the specific device. FPS can in-
tuitively measure the inference speed of the network. Param-
eters refers to the total number of parameters that must be
trained during model training. This metric is commonly used
to measure the size of a model. On resource-constrained
edge devices, the parameter volume is a critical factor to
consider. FLOPs is not chosen as one of the evaluation met-
rics because it can not directly estimate the inference speed.

Model Instantiation
We build our base model SCTNet-B with a comparable
size to that of RTFormer-B/DDRNet-23/STDC2. Further-
more, we also introduce a smaller variant called SCTNet-S.
The network architecture hyper-parameters of the two model

Table 9: Ablations on the semantic alignment loss, including types, locations and weight.

(a) Ablation on different types of loss.

(b) Ablation on the location of alignment loss.

(c) Ablation on the weight of alignment loss.

Loss Type Seg100 Seg75 Seg50
75.2
Baseline
73.0
L2 loss
KL loss
75.2
75.7
MI loss
75.7
CWD loss

77.5
76.8
77.6
79.0
79.1

78.5
77.8
78.6
79.7
79.7

logits decoder stage4 stage3 mIoU(%)‚Üë

‚úì
‚úì
‚úì
‚úì

‚úì
‚úì
‚úì

‚úì
‚úì

78.2
78.8(+0.6)
79.1(+0.9)
79.3(+1.1)
‚úì 79.7(+1.5)

Loss Weight Seg100 Seg75 Seg50
75.2
78.5
0,0,0,0
3,0,0,0
75.3
78.8
75.5
79.5
3,10,10,10
75.7
79.7
3,15,15,15
75.0
79.1
3,20,20,20

77.5
78.5
78.9
79.1
78.6

variants are as follows:
‚Ä¢ SCTNet-B: C = {64, 128, 256, 512}, layer numbers =

{2, 2, 3, 2}, N = 64, k = 7.

‚Ä¢ SCTNet-S: C = {32, 64, 128, 256}, layer numbers =

{2, 2, 3, 2}, N = 64, k = 7.

where C denotes the channel number of each stage, N is the
number of key/value in Conv-Former blocks, and k denotes
the kernel size of learnable kernels in Conv-Former blocks.
The two model variants utilize the same loss setting. We
use CE loss (Milletari, Navab, and Ahmadi 2016) and CWD
loss (Shu et al. 2021) as our overall training loss, which can
be formulated as follows:

Lall = ŒªmainLce + ŒªauxLce +

3
(cid:88)

i=0

cwdLi
Œªi

cwd

(5)

where Œªmain and Œªaux denote the loss weights of the de-
coder head and auxiliary head in stage 2, respectively. Œªcwd
denotes the loss weight of the context guidance head. We
set the hyper-parameters to Œªmain = 1.0, Œªaux = 0.4, and
Œªcwd = [3, 15, 15, 15] for output logits, features of decoder,
stage 4 and stage 3, respectively. Follow (Shu et al. 2021),
we set temperature T = 4 .

As for the semantic transformer branch in the training
phrase, it can be any hierarchical transformer. In our im-
plementation, we choose SegFormer (Xie et al. 2021) as
the transformer teacher for all experiments. Specifically, we
choose SegFormer-B3 for SCTNet-B and SegFormer-B2 for
SCTNet-S.

B More Ablation Experiments
Ablation Studies on the design of CFBlock
To further clarify the structure of our Conv-Former Block,
we conduct some ablation studies on the details of the
proposed convolutional attention module. Taking efficiency
into consideration, we implement the convolutional atten-
tion module with stripe convolution rather than standard
convolutions. More specifically, we utilize a 1√ó7 and a 7√ó1
convolution to approximate a 7√ó7 convolution layer. Table 8
shows the ablation studies on the kernel size of the learnable
kernels. As shown in Table 8, when kernel size is set to 7,
i.e., using a 1√ó7 and a 7√ó1 stripe convolutions, the SCTNet
reaches the highest mIoU, 1.0% higher than a single 1 √ó 1
convolution, while the FPS only decreases by 4.6. When the
kernel size is larger than 7, the mIoU stops to improve, but
the computational cost still increases. Therefore, we set the

hyper-parameter kernel size to 7 in all experiments. Note
that although the 7 √ó 7 standard convolution owns the slow-
est speed, it gets a lower mIoU compared with a 1 √ó 7 and
a 7 √ó 1 stripe convolutions. All experiments in Table 8 are
without pre-training.

Ablation Studies on Semantic Alignment Loss

We conduct extensive experiments on the types, locations,
and weights of the alignment loss.

The proposed semantic information alignment module
aims at instructing the CNN to learn the long-range seman-
tic information from the transformer. Therefore, the loss
which can supervise the learning of context information is
more suitable for our alignment loss. According to the re-
sults in Table 9(a), CWD loss has positive effects on the
performance, while L2 loss decreases the accuracy com-
pared with the baseline. At the same time, KL loss shows al-
most the same performance before the alignment. This phe-
nomenon could be attributed to the fact that CWD loss con-
verts the feature activation into channel-wise probability dis-
tribution so that it focuses on the overall information align-
ment in each channel, which may contain rich global con-
text. While L2 loss and KL loss directly align feature maps
in an element-wise manner without regarding feature maps
as an ensemble. Compared with KL loss, L2 loss adopts
harder alignment, leading to worse performance. MI loss
refers to mutual information loss, which is a measure of mu-
tual dependence between two random variables. We use the
technique of variational estimation in VID (Ahn et al. 2019)
to estimate the mutual information of the distribution of each
channel and achieve similar performance to CWD loss. Both
MI and CWD losses are applied to the channel-wise distribu-
tion. Therefore, what matters in our SCTNet is the alignment
of the distribution between channels rather than the specific
form of the similarity measurement. This is because SCTNet
aims to align the long-range context of the semantic branch,
which is contained in the overall channel distribution rather
than in each pixel or local area. Considering that the form
of CWD loss is simpler and easier for readers to understand,
we ultimately adopted CWD loss.

Table 9(b) shows the influence of the alignment loss lo-
cation. More long-range semantic information may exist in
deep stages of hierarchy CNN, and the results also indicate
that applying semantic alignment on stage3&4, decoder, and
output logits is the best setting for the mIoU performance.
Note that the location of alignment loss does not make any
difference in the inference FPS.

Table 10: Ablation Studies on Semantic Branch in Train-
ing Phrase.

Semantic Branch Seg100 Seg75 Seg50
76.4
OCRNet
76.5
InternImage
76.0
SwinTransformer
76.5
SegFormer

80.0
80.5
79.8
80.5

79.2
79.6
79.1
79.8

Figure 7: The speed-accuracy performance on COCO-
Stuff-10K test set.

Figure 6: The speed-accuracy performance on ADE20K
validation set.

At last, we explore the weight of alignment loss in Ta-
ble 9(c). For output logits, 3 is the best choice following (Shu
et al. 2021). For features, we find that 15 is the most suitable.
Smaller weights can not fully exploit the high-quality se-
mantic information of the transformer, while larger weights
compete with the CE loss. Moreover, we observe that when
the weights are too large, such as 50, the value of training
loss fluctuates dramatically, leading to low accuracy. Note
that all of these ablation studies are conducted without the
shared decoder head alignment.

Ablation Studies on Semantic Branch in Training
Phrase
We also conduct ablation experiments on different training-
only semantic branches to screen the appropriate ones. Ta-
ble 10 presents the influence of four types of semantic
branches: OCRNet, InternImage, Swin, and SegFormer. We
adopt these types of branches with similar accuracy, and the
training settings are the same. The consistent improvement
observed in the experiments demonstrates the effectiveness
of the proposed SIAM. Among the candidate branches, Seg-
Former and InternImage showed the most promising im-
provement. However, SegFormer requires less training time
and memory compared to others. Therefore, we selected
SegFormer as the final semantic branch for the training
phase.

C Comparison with More Methods
Trade-off on ADE20K and COCO-Stuff-10K
Most CNN-based real-time segmentation methods mainly
focus on Cityscapes (Cordts et al. 2016), while hardly tak-
ing ADE20K (Zhou et al. 2017) and COCO-Stuff-10K (Cae-
sar, Uijlings, and Ferrari 2018) into consideration. These

Figure 8: Speed-Accuracy performance on Cityscapes
validation set using a single NVIDIA RTX 2080Ti. Our
methods are presented in red starts, while others are pre-
sented in blue dots. Our SCTNet establishes a new state-of-
the-art speed-accuracy trade-off.

datasets contain numerous images with over 150 classes,
making them challenging for lightweight CNN-based mod-
els. However, recently some transformer-based real-time
segmentation methods have shown promising improvement
on ADE20K and COCO-Stuff-10K. Although our SCTNet
is a CNN-based network, it aligns the semantic information
with the transformer in the training phase. Therefore, the
SCTNet also shows competitive performance on ADE20K
and COCO-Stuff-10K. As present in Figure 6 and Figure 7,
our SCTNet outperforms other transformer-based and CNN-
based methods in real-time segmentation by a significant
margin.

Trade-off on RTX 2080Ti
To further demonstrate the high efficiency of our SCTNet on
different devices, we also measure the inference speed of our
SCTNet and the mentioned bilateral methods using a single
NVIDIA RTX 2080Ti. The results are presented in Table 11.
To clearly show the efficiency of our proposed SCTNet, we
also present the accuracy-speed comparison in Figure 8. Us-

SegFormerB0TopFormer-BSegNext-TAFFormer-BRTFormer-SRTFormer-BSeaFormer-BSCTNet-SSCTNet-B3638404244405060708090100110120130140150160Accuracy (mIoU%)Inference Speed (FPS)TopFormer-BSeaFormer-BAFFormer-BDDRNet23RTFormer-BSCTNet-B3233343536405060708090100110120130140150Accuracy (mIoU%)Inference Speed (FPS)STDC1-50STDC2-50 STDC1-75STDC2-75DDRNet-23-SDDRNet-23BiSeNet-ResNet18BiSeNetV2-LRTFormer-SRTFormer-BTopFormer-B-100TopFormer-B-50SeaFormer-B-100SeaFormer-B-50SCTNet-S-50SCTNet-S-75SCTNet-B-100SCTNet-B-75SCTNet-B-507071727374757677787980810255075100125150175200225250275300325350Accuracy (mIoU%)Inference Speed (FPS)Table 11: Comparisons with bilateral real-time methods on Cityscapes val set. All the speed is measured on a single RTX
2080Ti.

Method

Reference
CNN-based Bilateral Networks

BiSeNet-ResNet18
BiSeNetV2-L
STDC1-Seg75
STDC2-Seg75
STDC1-Seg50
STDC2-Seg50
DDRNet-23-S
DDRNet-23

ECCV 2018
IJCV 2021
CVPR 2021
CVPR 2021
CVPR 2021
CVPR 2021
TIP 2022
TIP 2022

Transformer-based Bilateral Networks
TopFormer-B-Seg100 CVPR 2022b
CVPR 2022b
TopFormer-B-Seg50
ICLR 2023
SeaFormer-B-Seg100
ICLR 2023
SeaFormer-B-Seg50
NeurIPS 2022
RTFormer-S
NeurIPS 2022
RTFormer-B
SCTNet-S-Seg50
Ours
SCTNet-S-Seg75
Ours
SCTNet-B-Seg50
Ours
SCTNet-B-Seg75
Ours
SCTNet-B-Seg100
Ours

#Params‚Üì

Resolution

FPS(TRT)‚Üë

FPS(Torch)‚Üë mIoU(%)‚Üë

49.0M
-
14.2M
22.2M
14.2M
22.2M
5.7M
20.1M

5.1M
5.1M
8.6M
8.6M
4.8M
16.8M
4.6M
4.6M
17.4M
17.4M
17.4M

1536 √ó 768
1024 √ó 512
1536 √ó 768
1536 √ó 768
1024 √ó 512
1024 √ó 512
2048 √ó 1024
2048 √ó 1024

2048 √ó 1024
1024 √ó 512
2048 √ó 1024
1024 √ó 512
2048 √ó 1024
2048 √ó 1024
1024 √ó 512
1536 √ó 768
1024 √ó 512
1536 √ó 768
2048 √ó 1024

112.3
70.3
140.9
104.1
290.7
236.1
93.2
70.2

85.3
310.2
77.6
223.5
-
-
327.5
156.6
244.4
115.2
76.7

72.4
35.2
72.2
57.2
143.6
89.8
85.5
36.7

64.2
90.8
36.7
43.9
70.9
30.8
225.1
147.9
136.9
65.7
42.0

74.8
75.8
74.5
77.0
72.2
74.2
77.8
79.5

76.3
70.7
77.7
72.2
76.3
79.3
72.8
76.1
76.5
79.8
80.5

ing the same NVIDIA RTX 2080Ti, our SCTNet-B also out-
performs existing bilateral real-time semantic segmentation
networks by a large margin. At the same time, our SCTNet-
S keeps a competitive trade-off of speed and accuracy with
these methods. Note that, TensorRT does not support some
operations used in the RTFormer (Wang et al. 2022), but the
acceleration details of RTFormer are not available. So we
report its original speed in Figure 8, which is also deployed
on RTX 2080Ti.

More Methods with Specific Devices
In the main paper, we measured all speeds on a single RTX
3090 for fair comparison. And because of space limitations,
we only compare recent state-of-the-art works. Therefore,
we perform a more comprehensive comparison in Table 12.
Note that all speeds mentioned here are obtained from the
original paper with corresponding devices.

D Disscussions
Other Kinds of Real-time Semantic Segmentation
Methods.
Our motivation is accelerating and improving the bilateral-
branch networks for real-time semantic segmentation. So we
focus on introducing the development of the bilateral-branch
real-time semantic segmentation networks in related work,
helping readers to understand our work in a limited space
quickly. However, there are some other types of methods in
that field. In order to avoid omissions, we discuss more com-
prehensive related work here.
Single-branch Networks. There are some works focusing
on single-branch real-time segmentation networks. How-

ever, lightweight single-branch networks usually lack long-
range semantic information, leading to low accuracy. To
capture high-quality semantic information and achieve high
performance, AFFormer (Bo, Pichao, and Wang 2023) use
complex encoder blocks, SegNext (Guo et al. 2022b) stacks
lots of multi-scale convolution layers, SFNet (Li et al.
2022b) refers to a heavy decoder. All these results in larger
inference latency, weakening the speed advantage of the
original lightweight single-branch network.
Multi-branch Networks. Multi-branch networks extend
bilateral networks to more branches. For instance, PID-
Net (Xu, Xiong, and Bhattacharyya 2023) adopts a hierar-
chical main branch along with a detailed branch and a se-
mantic branch. While multi-branch designs can enhance per-
formance, they also come with higher computational costs,
resulting in slower inference speeds compared to two-branch
methods.

Difference from MSCA.
There is also a convolutional attention in SegNext (Guo
et al. 2022b), called MSCA(Multi-Scale Convolutional At-
tention). Although both the MSCA and the convolutional at-
tention in our proposed CFBlock consist of several convo-
lutions, there are very different. MSCA utilizes multi-scale
stripe convolutions to capture multi-scale context, and then
the extracted context feature map serves as the weight map
to reweigh the origin feature map. Our convolutional atten-
tion utilizes external kernels as key and value, calculates
the similarity between the external kernels and the feature
map, and uses the value of external kernels to enhance the
feature map. MSCA contains multi-scale convolution lay-
ers and larger kernels(like kernel size=21), resulting in more

Table 12: More comprehensive accuracy and speed performance comparison on Cityscapes val set. All the speed in this
table is from the original paper with corresponding devices. ‚àó means it uses some acceleration methods, not the original torch
speed.

Method
ERFNet
Fast-SCNN
CAS
GAS
FaPN-R34
AFFormer-B
FasterSeg
SwiftNetRN-18
PP-LiteSeg-T2
PP-LiteSeg-B2
DF2-Seg1
DF2-Seg2

GPU
TitanX M
TitanXp
TitanXp
TitanXp
Titan RTX
V100
GTX1080Ti
GTX1080Ti
GTX1080Ti
GTX1080Ti
GTX1080Ti
GTX1080Ti
BiSeNet-ResNet18 GTX1080Ti
GTX1080Ti
BiSeNetV2-L
STDC1-Seg75
GTX1080Ti
GTX1080Ti
STDC2-Seg75
GTX1080Ti
STDC1-Seg50
GTX1080Ti
STDC2-Seg50
RTX 2080Ti
DDRNet-23-S
RTX 2080Ti
DDRNet-23
RTX 2080Ti
RTFormer-S
RTX 2080Ti
RTFormer-B
RTX 3090
SFNet-ResNet18
RTX 3090
PIDNet-S
RTX 3090
PIDNet-M
SCTNet-S-Seg50
RTX 3090
SCTNet-S-Seg75
RTX 3090
SCTNet-B-Seg50
RTX 3090
SCTNet-B-Seg75
RTX 3090
SCTNet-B-Seg100 RTX 3090

#Params‚Üì
20M
1.1M
-
-
-
3.0M
4.4M
11.8M
-
-
-
-
49.0M
-
14.2M
22.2M
14.2M
22.2M
5.7M
20.1M
4.8M
16.8M
12.3M
7.6M
34.4M
4.6M
4.6M
17.4M
17.4M
17.4M

Resolution
2048 √ó 1024
2048 √ó 1024
1536 √ó 768
1536 √ó 768
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
1536 √ó 768
1536 √ó 768
1536 √ó 768
1536 √ó 768
1536 √ó 768
1024 √ó 512
1536 √ó 768
1536 √ó 768
1024 √ó 512
1024 √ó 512
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
2048 √ó 1024
1024 √ó 512
1536 √ó 768
1024 √ó 512
1536 √ó 768
2048 √ó 1024

FPS(TRT)‚Üë
-
-
-
-
-
-
163.9
-
143.6
102.6
67.2
56.3
-
47.3
126.7
97.0
250.4
188.6
-
-
-
-
50.5
-
-
451.2
233.3
374.6
186.6
105.0

FPS(Torch)‚Üë mIoU(%)‚Üë

41.7
123.5
108
108.4
30.2
22
-
104.0
-
-
-
-
65.5
-
-
-
-
-
101.6
37.1
110.0‚àó
39.1‚àó
24.0
93.2
39.8
160.3
149.2
144.9
105.2
62.8

70.0
68.6
71.6
72.4
78.5
78.7
73.1
75.5
76.0
78.2
75.9
76.9
74.8
75.8
74.5
77.0
72.2
74.2
77.8
79.5
76.3
79.3
79.0
78.8
80.1
72.8
76.1
76.5
79.8
80.5

computational costs. As shown in Table 4(main paper), our
CFBlock shows similar performance with fewer parameters
and faster speed than MSCAN.

segmentation, the promising performance of our combined
use of SIMA and CFBlock also provides insights for hetero-
geneous distillation from transformer to CNN.

Knowledge Distillation for Isomeric Models.
Knowledge distillation was first proposed by Hinton et al.
(Hinton, Vinyals, and Dean 2015). Most distillation meth-
ods consider only homologous structures of teacher and stu-
dent. Recently, some methods (Touvron et al. 2021; Li et al.
2022a) conduct distillation between CNN teacher and trans-
former student to tackle the problem of missing local infor-
mation on small datasets for the transformer. Liu et al. (Liu
et al. 2022) studies distillation between transformer teacher
and CNN student in classification. In the field of semantic
segmentation, Zhu et al. (Zhu et al. 2023) introduced a col-
laborative learning process that improves the performance
of CNN and transformer models. In contrast, our approach
focuses on leveraging the high-quality semantic information
of the transformer as a semantic branch to assist the learning
of a single-branch CNN. As a result, we do not require joint
training and have a smaller training cost. While our primary
goal is to use the transformer to boost real-time semantic

Limitations and Future work
We demonstrate the effectiveness of our SCTNet on vari-
ous GPU devices and datasets, both on torch and Tensor-RT
models. However, we do not report CPU latency on mobile
devices. Building a consistent benchmark for CPU latency
and evaluating so many methods is beyond the scope of this
paper. Additionally, the design of SCTNet does not include
special modifications for CPU, such as replacing convolu-
tion operations with depth separable convolution or using
ReLU6 instead of ReLU. We plan to modify SCTNet for
CPU usage in future work.

While we have achieved a new state-of-the-art trade-off
between accuracy and speed on various datasets, there is
potential to further improve performance by scaling up our
SCTNet to SCTNet-L. This is because our SCTNet-B ex-
ceeds the real-time standard(30FPS) by 32.8 FPS. We will
explore this in future work and update our results in subse-
quent versions of this paper.

e
g
a
m

I

)
a
(

T
G

)
b
(

0
B

-
r
e
m
r
o
F
g
e
S
)
c
(

B

-
r
e
m
r
o
F
T
R

)
d
(

B

-
r
e
m
r
o
F
p
o
T
)
e
(

B

-
r
e
m
r
o
F
a
e
S
)
f
(

B

-
t
e
N
T
C
S
)
g
(

Figure 9: Visualization results on ADE20K validation set. Compared with SegFormer-B0 (Xie et al. 2021), RTFormer-
B (Wang et al. 2022), TopFormer-B (Zhang et al. 2022b) and SeaFormer-B(Wan et al. 2023), SCTNet-B generates masks with
finer details as highlighted in the dark blue box and more accurate large-area predictions, as highlighted in the yellow box.

(a) Image

(b) GT

(c) SeaFormer-B

(d) STDC2

(e) SCTNet-B

Figure 10: Visualization results on Cityscapes validation set. Compared with BiSeNetV2-L(Yu et al. 2021) and RTFormer-
B (Wang et al. 2022), SCTNet-B generates masks with finer details as highlighted in the light blue box and more accurate
large-area predictions, as highlighted in the yellow box.

E More Visualization Results
In this section, we show more visualization results on
ADE20K (Zhou et al. 2017) and Cityscapes (Cordts et al.
2016) to demonstrate the effectiveness of the SCTNet intu-
itively.

For ADE20K (Zhou et al. 2017), Figure 9 presents the
visualization results of all the mentioned transformer-based
real-time segmentation methods and our SCTNet-B. In the
first column, our SCTNet segments the buildings with bet-
ter internal consistency and has a better mask for the tree. In
the second column, our SCTNet has smooth boundaries of
the curtain and the cushion like the GT masks. In the third
column, our SCTNet generates better segmentation of the
armchair and has fewer wrong masks for the painting. In
the fourth column, our SCTNet generates better masks for
fences and poles. These visualized results show that SCTNet
has better capability to distinguish different classes when
compared with SegFormer-B0 (Xie et al. 2021), RTFormer-
B (Wang et al. 2022), TopFormer-B (Zhang et al. 2022b) and
SeaFormer-B(Wan et al. 2023).

Moreover, we provide more visualization results on
Cityscapes (Cordts et al. 2016) in Figure 10, which are
not present in the paper text because of the page limi-
tation. Compared with SeaFormer (Wan et al. 2023) and
STDC (Fan et al. 2021), our SCTNet provides not only better
results for those classes with large areas like road and side-
walk, but also more accurate boundaries for small or thin
objects such as poles, traffic lights, traffic signs, and cars.
This indicates that SCTNet extracts high-quality long-range
context while preserving fine details.

